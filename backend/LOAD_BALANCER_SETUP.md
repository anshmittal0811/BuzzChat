# Load Balanced Chat Services Setup

This guide explains how to run **3 chat service instances** with an **nginx load balancer** to handle 30,000+ concurrent WebSocket connections.

## ğŸ¯ What This Achieves

- **3x Connection Capacity**: From ~10,000 to ~30,000+ concurrent WebSocket connections
- **High Availability**: If one chat instance fails, others continue serving
- **Better Performance**: Load is distributed across multiple instances
- **Sticky Sessions**: Users stay connected to the same instance during their session

## ğŸ“ Architecture Overview

```
Frontend Clients
       â†“
Load Balancer (nginx:3001)
       â†“
    â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
    â”‚     â”‚     â”‚     â”‚
Chat-1  Chat-2  Chat-3
(10K)   (10K)   (10K) WebSocket connections
    â”‚     â”‚     â”‚     â”‚
    â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
          â†“
   Shared Infrastructure
   (MongoDB, Redis, Kafka)
```

## ğŸš€ Quick Start

### 1. Start the Load Balanced Setup
```bash
# Navigate to backend directory
cd backend

# Start all services (this will start 3 chat instances + load balancer)
./docker-start.sh start
```

### 2. Verify Everything is Running
```bash
# Check status of all services
./docker-start.sh status

# Check health of each chat instance
./docker-start.sh health

# View scaling information
./docker-start.sh scale
```

### 3. Update Your Frontend
Your frontend should connect to the load balancer:
```javascript
// OLD: Direct connection
const socket = io("http://localhost:3001");

// NEW: Connection through load balancer (same URL!)
const socket = io("http://localhost:3001");
```

**No frontend changes needed!** The load balancer is transparent.

## ğŸ”§ Configuration Details

### Docker Compose Changes

#### Chat Service Instances
```yaml
# 3 identical chat services with resource limits
chat-service-1:
  build: ...
  container_name: chat-service-1
  deploy:
    resources:
      limits:
        memory: 512M
        cpus: '0.5'

chat-service-2: # Same config
chat-service-3: # Same config
```

#### Load Balancer
```yaml
chat-load-balancer:
  image: nginx:alpine
  ports:
    - "3001:80"  # External traffic hits this
  volumes:
    - ./nginx.conf:/etc/nginx/nginx.conf:ro
```

### Nginx Configuration (nginx.conf)

#### Key Features:
- **IP Hash Load Balancing**: Ensures sticky sessions
- **WebSocket Support**: Proper upgrade headers
- **Health Checks**: Automatic failover if instance fails
- **Optimized Timeouts**: Long-lived connections supported

```nginx
upstream chat_backend {
    ip_hash;  # Sticky sessions for WebSocket
    
    server chat-service-1:3001 max_fails=3 fail_timeout=30s;
    server chat-service-2:3001 max_fails=3 fail_timeout=30s;
    server chat-service-3:3001 max_fails=3 fail_timeout=30s;
}

server {
    location / {
        proxy_pass http://chat_backend;
        
        # WebSocket support
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        
        # Long timeouts for persistent connections
        proxy_read_timeout 3600s;
    }
}
```

## ğŸ“Š Performance Metrics

### Current Capacity (3 Instances)
- **Concurrent WebSocket Connections**: ~30,000-45,000
- **Messages per Second**: ~15,000-25,000
- **Memory Usage**: ~1.5GB (512MB Ã— 3 instances)
- **CPU Usage**: ~1.5 cores (0.5 Ã— 3 instances)

### Bottleneck Analysis
- **Before**: Single chat service = 10,000 connections max
- **After**: 3 chat services = 30,000+ connections
- **Next Bottleneck**: Database connections (MongoDB)

## ğŸ›ï¸ Monitoring & Health Checks

### Built-in Health Checks
```bash
# Check individual instances
./docker-start.sh health

# Check specific service logs
./docker-start.sh logs chat-service-1
./docker-start.sh logs chat-load-balancer

# Monitor all chat services
./docker-start.sh logs | grep chat
```

### Manual Health Checks
```bash
# Test load balancer
curl http://localhost:3001/health

# Test specific instances (inside containers)
docker-compose exec chat-service-1 wget -qO- http://localhost:3001/health
docker-compose exec chat-service-2 wget -qO- http://localhost:3001/health
docker-compose exec chat-service-3 wget -qO- http://localhost:3001/health
```

## ğŸ”„ How Load Balancing Works

### Connection Distribution
1. **Client connects** to `ws://localhost:3001`
2. **Nginx receives** the connection
3. **IP Hash algorithm** determines which instance to use
4. **Sticky session** ensures user stays on same instance
5. **WebSocket upgrade** is properly proxied

### Failover Behavior
- If `chat-service-1` fails â†’ nginx stops sending traffic there
- Existing connections to failed instance are lost (clients reconnect)
- New connections are distributed among healthy instances
- Service automatically rejoins when it recovers

## âš™ï¸ Scaling Further

### Add More Instances

1. **Add to docker-compose.yml:**
```yaml
chat-service-4:
  build:
    context: .
    dockerfile: Dockerfile
    args:
      APP_NAME: chat
  container_name: chat-service-4
  # ... same config as others
```

2. **Update nginx.conf:**
```nginx
upstream chat_backend {
    ip_hash;
    server chat-service-1:3001;
    server chat-service-2:3001;
    server chat-service-3:3001;
    server chat-service-4:3001;  # Add new instance
}
```

3. **Restart services:**
```bash
./docker-start.sh build
./docker-start.sh restart
```

### Vertical Scaling (More Resources)
```yaml
deploy:
  resources:
    limits:
      memory: 1G      # Increase from 512M
      cpus: '1.0'     # Increase from 0.5
```

### Database Scaling
When you reach ~50,000 connections, consider:
- MongoDB replica sets
- Redis clustering
- Connection pooling optimization

## ğŸ“ˆ Expected Load Test Results

### Before (Single Instance)
- 10,000 connections â†’ Service degrades
- 15,000 connections â†’ Service fails

### After (3 Instances + Load Balancer)
- 30,000 connections â†’ Good performance
- 45,000 connections â†’ Some degradation
- 50,000+ connections â†’ Need database optimization
